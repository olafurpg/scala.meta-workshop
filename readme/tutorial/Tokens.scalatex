@import scalaworld.Tutorial._
@import scalaworld.Readme._

@sect{Tokens}
  @p
    Make sure you have Scalameta installed as a library from @sect.ref{Setup}.

    You can decide to run these examples from the console or from sbt,
    for example in the tutorial repo.

  @p
    This whole workshop will assume you have this import in scope:

  @repl
    import scala.meta._

  @p
    Here's how to tokenize a small expression.

    @meta
      "val x = 2".tokenize.get

    Let's discuss the most interesting methods on tokens.

    @sect{Tokens.syntax}
      @p
        The simplest method we can call is @code{Tokens.syntax}
        The method returns a string representation of the actual code behind
        the tokens, or how the code should look like to a developer.

      @meta
        "val x = 2".tokenize.get.syntax


      @p
        @code{Tokens.toString()} uses @code{.syntax} behind the scenes.
        However, you should never rely on @code{toString()} when manipulating
        Scalameta structures, prefer to explicitly call @code{.syntax}.
        It's maybe not so obvious why right now but it will make more sense soon.

    @sect{Tokens.structure}
      @p
        Another useful method is @code{Tokens.structure}.
        The method shows details that may be relevant to us as metaprogrammers.

        @meta
          "val x = 2".tokenize.get.structure

      @p
        @code{.structure} is often useful for debugging and testing.

    @sect{Tokens vs. Token}
      @p
        The class @code{Tokens} is a wrapper around a sequence of @code{Token} objects.
        There are multiple subtypes of @code{Token} while there only one type @code{Tokens}.

        @meta
          "val x = 2".tokenize.get.head

       @code{BOF} stands for "Beginning of file".
       Let's see what other kinds of token types are in the string

        @meta
          "val x = 2".tokenize.get.
            map(x => f"${x.structure}%10s -> ${x.productPrefix}").
            mkString("\n")

        @p
          Even spaces get their own tokens.
          The @code{[0...3)} part indicates that the @code{val} tokens start at
          offset 0 and ends at offset 3.

    @sect("==", "Tokens.==")
      @p
        How does token equality look like?

      @meta
        "foobar".tokenize.get(1) == "foobar kas".tokenize.get(1)

      Huh, why are they not the same?

      @warning
        Token equality is implemented with reference equality.
        You need to be explicit if you actually mean syntactic (@code{.syntax}),
        or structural (@code{.structure}) equality.

      The tokens are syntactically equal.

      @meta
        "foobar".tokenize.get(1).syntax == "foobar kas".tokenize.get(1).syntax

      Even if we move the tokens around

      @meta
        "kas foobar".tokenize.get(3).syntax == "foobar kas".tokenize.get(1).syntax

      The tokens are also structurally equal.

      @meta
        "foobar".tokenize.get(1).structure == "foobar kas".tokenize.get(1).structure

      However, they are not structurally equal if we move them around.

      @meta
        "kas foobar".tokenize.get(3).structure == "foobar kas".tokenize.get(1).structure

    @sect(".get", "Tokens.get")
      @p
        Tokenization can sometimes fail, for example in this case:

        @meta
          """ val str = "unclosed literal """.tokenize // compilation error

        @p
          If you prefer, you can safely pattern match on the tokenize result

          @meta
            """ val str = "closed literal" """.tokenize match {
              case tokenizers.Tokenized.Success(tokenized) => tokenized
              case tokenizers.Tokenized.Error(e, _, _) => ???
            }

  @sect("Conclusion", "Token Conclusions")
    @p
      Scalameta tokens are the foundation of Scalameta.
      Sometimes you don't have access to a parsed AST and then your best shot is
      work with tokens.

    @p
      In the following chapter we will discuss another exciting data structure:
      the incredible @b{scala.meta.Tree}.
